# -*- coding: utf-8 -*-
"""COA_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1htBE2TgnpfL-oFIWkMbV1h2IYsBBzeF6
"""

pip install -U "ray[data,train,tune,serve]"

import ray
@ray.remote
def square(x):
	return x*x

#ray.remote decorator to function you want to use remotely
#this gives a call to future object which is like a reference to the function's result

futures=[square.remote(i) for i in range(4)]

#get used to retrieve results
print(ray.get(futures))

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]

plt.scatter(x, y)
plt.show()

data = list(zip(x, y))
inertias = []


for i in range(1,11):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(data)
    inertias.append(kmeans.inertia_)

plt.plot(range(1,11), inertias, marker='o')
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

# K-Means Clustering
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import ray
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import KernelPCA

# from google.colab import files
# uploaded = files.upload()




# Start Ray
ray.init(ignore_reinit_error=True)

# Importing the dataset with correct file path
# dataset = pd.read_csv(r'C:\Users\ADMIN\Downloads\crime.csv')
# Now load the CSV using the uploaded file
dataset = pd.read_csv("crime.csv")
X = dataset.iloc[:, [1,2,3,4,5,6,7,12]].values

# Feature Scaling
sc_X = StandardScaler()
X = sc_X.fit_transform(X)

# Using the elbow method to find the optimal number of clusters
wcss = []
for i in range(1, 21):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plotting the Elbow method result
plt.plot(range(1, 21), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Define the K-Means function as a Ray remote function
@ray.remote
def run_kmeans(X, n_clusters=6, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_

# Run the K-Means clustering remotely
future_result = run_kmeans.remote(X, 6)

# Get the result from the Ray task
y_kmeans, cluster_centers = ray.get(future_result)

# Applying KernelPCA for dimensionality reduction
#KernelPCA:-non linear dimensionality reduction

kpca = KernelPCA(n_components=2, kernel='rbf')
X = kpca.fit_transform(X)

# Visualizing the clusters
plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s=100, c='magenta', label='Cluster 5')
plt.scatter(X[y_kmeans == 5, 0], X[y_kmeans == 5, 1], s=100, c='black', label='Cluster 6')

# Plotting the centroids
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], s=300, c='yellow', label='Centroids')
plt.title('Clusters of areas')
plt.xlabel('Factor 1')
plt.ylabel('Factor 2')
plt.legend()
plt.show()

# Shutdown Ray
ray.shutdown()

# K-Means Clustering
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import ray
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import KernelPCA

# from google.colab import files
# uploaded = files.upload()




# Start Ray
ray.init(ignore_reinit_error=True)

# Importing the dataset with correct file path
# dataset = pd.read_csv(r'C:\Users\ADMIN\Downloads\crime.csv')
# Now load the CSV using the uploaded file
dataset = pd.read_csv("crime.csv")
X = dataset.iloc[:, [1,2,3,4,5,6,7,12]].values

# Feature Scaling
sc_X = StandardScaler()
X = sc_X.fit_transform(X)

# Using the elbow method to find the optimal number of clusters
wcss = []
for i in range(1, 21):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plotting the Elbow method result
plt.plot(range(1, 21), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Define the K-Means function as a Ray remote function
@ray.remote
def run_kmeans(X, n_clusters=6, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_

# Run the K-Means clustering remotely
future_result = run_kmeans.remote(X, 6)

# Get the result from the Ray task
y_kmeans, cluster_centers = ray.get(future_result)

# Applying KernelPCA for dimensionality reduction
#KernelPCA:-non linear dimensionality reduction

kpca = KernelPCA(n_components=2, kernel='rbf')
X = kpca.fit_transform(X)

# Visualizing the clusters
plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s=100, c='magenta', label='Cluster 5')
plt.scatter(X[y_kmeans == 5, 0], X[y_kmeans == 5, 1], s=100, c='black', label='Cluster 6')

# Plotting the centroids
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], s=300, c='yellow', label='Centroids')
plt.title('Clusters of areas')
plt.xlabel('Factor 1')
plt.ylabel('Factor 2')
plt.legend()
plt.show()

# Shutdown Ray
ray.shutdown()

# K-Means Clustering
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import ray
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import KernelPCA

# from google.colab import files
# uploaded = files.upload()




# Start Ray
ray.init(ignore_reinit_error=True)

# Importing the dataset with correct file path
# dataset = pd.read_csv(r'C:\Users\ADMIN\Downloads\crime.csv')
# Now load the CSV using the uploaded file
dataset = pd.read_csv("crime.csv")
X = dataset.iloc[:, [1,2,3,4,5,6,7,12]].values

# Feature Scaling
sc_X = StandardScaler()
X = sc_X.fit_transform(X)

# Using the elbow method to find the optimal number of clusters
wcss = []
for i in range(1, 21):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plotting the Elbow method result
plt.plot(range(1, 21), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Define the K-Means function as a Ray remote function
@ray.remote
def run_kmeans(X, n_clusters=6, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_

# Run the K-Means clustering remotely
future_result = run_kmeans.remote(X, 6)

# Get the result from the Ray task
y_kmeans, cluster_centers = ray.get(future_result)

# Applying KernelPCA for dimensionality reduction
#KernelPCA:-non linear dimensionality reduction

kpca = KernelPCA(n_components=2, kernel='rbf')
X = kpca.fit_transform(X)

# Visualizing the clusters
plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s=100, c='magenta', label='Cluster 5')
plt.scatter(X[y_kmeans == 5, 0], X[y_kmeans == 5, 1], s=100, c='black', label='Cluster 6')

# Plotting the centroids
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], s=300, c='yellow', label='Centroids')
plt.title('Clusters of areas')
plt.xlabel('Factor 1')
plt.ylabel('Factor 2')
plt.legend()
plt.show()

# Shutdown Ray
ray.shutdown()

# K-Means Clustering
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import ray
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import KernelPCA

# from google.colab import files
# uploaded = files.upload()




# Start Ray
ray.init(ignore_reinit_error=True)

# Importing the dataset with correct file path
# dataset = pd.read_csv(r'C:\Users\ADMIN\Downloads\crime.csv')
# Now load the CSV using the uploaded file
dataset = pd.read_csv("crime.csv")
X = dataset.iloc[:, [1,2,3,4,5,6,7,12]].values

# Feature Scaling
sc_X = StandardScaler()
X = sc_X.fit_transform(X)

# Using the elbow method to find the optimal number of clusters
wcss = []
for i in range(1, 21):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plotting the Elbow method result
plt.plot(range(1, 21), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Define the K-Means function as a Ray remote function
@ray.remote
def run_kmeans(X, n_clusters=6, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_

# Run the K-Means clustering remotely
future_result = run_kmeans.remote(X, 6)

# Get the result from the Ray task
y_kmeans, cluster_centers = ray.get(future_result)

# Applying KernelPCA for dimensionality reduction
#KernelPCA:-non linear dimensionality reduction

kpca = KernelPCA(n_components=2, kernel='rbf')
X = kpca.fit_transform(X)

# Visualizing the clusters
plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s=100, c='magenta', label='Cluster 5')
plt.scatter(X[y_kmeans == 5, 0], X[y_kmeans == 5, 1], s=100, c='black', label='Cluster 6')

# Plotting the centroids
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], s=300, c='yellow', label='Centroids')
plt.title('Clusters of areas')
plt.xlabel('Factor 1')
plt.ylabel('Factor 2')
plt.legend()
plt.show()

# Shutdown Ray
ray.shutdown()

import multiprocessing
print("Number of CPU cores available:", multiprocessing.cpu_count())

import time
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import ray
import random
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import KernelPCA
from sklearn.impute import SimpleImputer



# from google.colab import files
# uploaded = files.upload()

# Start Ray
ray.init(ignore_reinit_error=True)

# Load dataset
dataset = pd.read_csv("Crime_Data_from_2020_to_Present.csv")
numeric_cols = dataset.select_dtypes(include=np.number).columns.tolist()
X = dataset[numeric_cols].values
#X = dataset.iloc[:, [1, 2, 3, 4, 5, 6, 7, 12]].values

imputer = SimpleImputer(strategy='mean')  # Or 'median', 'most_frequent'
X = imputer.fit_transform(X)
# Feature Scaling
sc_X = StandardScaler()
X = sc_X.fit_transform(X)

# Using the elbow method to find the optimal number of clusters
wcss = []


for i in range(1, 21):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)





# Plotting the Elbow method result
plt.plot(range(1, 21), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Serial execution of K-Means
def run_kmeans_serial(X, n_clusters=12, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_

# Parallel execution of K-Means using Ray
@ray.remote
def run_kmeans_parallel(X, n_clusters=12, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_
# Measure serial execution time
start_time_serial = time.time()
y_kmeans_serial, cluster_centers_serial = run_kmeans_serial(X, 6)
end_time_serial = time.time()

serial_execution_time = end_time_serial - start_time_serial
print(f"Serial Execution Time: {serial_execution_time:.4f} seconds")

# Measure parallel execution time
start_time_parallel = time.time()
future_result = run_kmeans_parallel.remote(X, 6)
y_kmeans_parallel, cluster_centers_parallel = ray.get(future_result)
end_time_parallel = time.time()

parallel_execution_time = end_time_parallel - start_time_parallel
print(f"Parallel Execution Time: {parallel_execution_time:.4f} seconds")

# Applying KernelPCA for dimensionality reduction
kpca = KernelPCA(n_components=2, kernel='rbf')
X_transformed = kpca.fit_transform(X)

# Visualizing the clusters
plt.scatter(X_transformed[y_kmeans_serial == 0, 0], X_transformed[y_kmeans_serial == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X_transformed[y_kmeans_serial == 1, 0], X_transformed[y_kmeans_serial == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X_transformed[y_kmeans_serial == 2, 0], X_transformed[y_kmeans_serial == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(X_transformed[y_kmeans_serial == 3, 0], X_transformed[y_kmeans_serial == 3, 1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X_transformed[y_kmeans_serial == 4, 0], X_transformed[y_kmeans_serial == 4, 1], s=100, c='magenta', label='Cluster 5')
plt.scatter(X_transformed[y_kmeans_serial == 5, 0], X_transformed[y_kmeans_serial == 5, 1], s=100, c='black', label='Cluster 6')

# Plotting the centroids
plt.scatter(cluster_centers_serial[:, 0], cluster_centers_serial[:, 1], s=300, c='yellow', label='Centroids')
plt.title('Clusters of areas (Serial Execution)')
plt.xlabel('Factor 1')
plt.ylabel('Factor 2')
plt.legend()
plt.show()

import time
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import concurrent.futures
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import KernelPCA

# Load dataset
dataset = pd.read_csv("crime.csv")
X = dataset.iloc[:, [1, 2, 3, 4, 5, 6, 7, 12]].values

# Feature Scaling
sc_X = StandardScaler()
X = sc_X.fit_transform(X)

# Using the elbow method to find the optimal number of clusters
wcss = []
for i in range(1, 21):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plotting the Elbow method result
plt.plot(range(1, 21), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Serial execution of K-Means
def run_kmeans_serial(X, n_clusters=6, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_

# Parallel execution of K-Means using concurrent.futures
def run_kmeans_parallel(X, n_clusters=6, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_

# Measure serial execution time
start_time_serial = time.time()
y_kmeans_serial, cluster_centers_serial = run_kmeans_serial(X, 6)
end_time_serial = time.time()

serial_execution_time = end_time_serial - start_time_serial
print(f"Serial Execution Time: {serial_execution_time:.4f} seconds")

# Measure parallel execution time using ProcessPoolExecutor
start_time_parallel = time.time()
with concurrent.futures.ProcessPoolExecutor() as executor:
    future_result = executor.submit(run_kmeans_parallel, X, 6)
    y_kmeans_parallel, cluster_centers_parallel = future_result.result()
end_time_parallel = time.time()

parallel_execution_time = end_time_parallel - start_time_parallel
print(f"Parallel Execution Time: {parallel_execution_time:.4f} seconds")

# Applying KernelPCA for dimensionality reduction
kpca = KernelPCA(n_components=2, kernel='rbf')
X_transformed = kpca.fit_transform(X)

# Visualizing the clusters
plt.scatter(X_transformed[y_kmeans_serial == 0, 0], X_transformed[y_kmeans_serial == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X_transformed[y_kmeans_serial == 1, 0], X_transformed[y_kmeans_serial == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X_transformed[y_kmeans_serial == 2, 0], X_transformed[y_kmeans_serial == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(X_transformed[y_kmeans_serial == 3, 0], X_transformed[y_kmeans_serial == 3, 1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X_transformed[y_kmeans_serial == 4, 0], X_transformed[y_kmeans_serial == 4, 1], s=100, c='magenta', label='Cluster 5')
plt.scatter(X_transformed[y_kmeans_serial == 5, 0], X_transformed[y_kmeans_serial == 5, 1], s=100, c='black', label='Cluster 6')

# Plotting the centroids
plt.scatter(cluster_centers_serial[:, 0], cluster_centers_serial[:, 1], s=300, c='yellow', label='Centroids')
plt.title('Clusters of areas (Serial Execution)')
plt.xlabel('Factor 1')
plt.ylabel('Factor 2')
plt.legend()
plt.show()

import time
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import concurrent.futures
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import KernelPCA

dataset=pd.read_csv("Crime_Data_from_2020_to_Present.csv")
dataset.head()

dataset.shape

dataset.columns

X = dataset[['Vict Age', 'LAT', 'LON']].dropna().values  # Drop rows with NaN values

# Feature Scaling
sc_X = StandardScaler()
X_scaled = sc_X.fit_transform(X)

# Using the elbow method to find the optimal number of clusters
wcss = []
for i in range(1, 21):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plotting the Elbow method result
plt.plot(range(1, 21), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Serial execution of K-Means
'''def run_kmeans_serial(X, n_clusters=3, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_
'''

# Parallel execution of K-Means using concurrent.futures
'''def run_kmeans_parallel(X, n_clusters=3, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_
'''

def run_kmeans_serial(X, n_clusters, init, random_state):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_

from sklearn.cluster import KMeans
def run_kmeans(X, n_clusters, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    kmeans.fit(X)
    return kmeans.labels_, kmeans.cluster_centers_

# Parallel execution of K-Means using concurrent.futures
def run_kmeans_parallel(X, n_clusters_list, init='k-means++', random_state=42):
    if isinstance(n_clusters_list, int):
        n_clusters_list = [n_clusters_list]

    results = []
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [
            executor.submit(run_kmeans, X, n_clusters, init, random_state)
            for n_clusters in n_clusters_list
        ]
        for future in concurrent.futures.as_completed(futures):
            results.append(future.result())

    # Check that results is structured as expected
    return results

# Measure serial execution time
start_time_serial = time.time()
y_kmeans_serial, cluster_centers_serial = run_kmeans_serial(X_scaled, 6,init='k-means++', random_state=42)
end_time_serial = time.time()

serial_execution_time = end_time_serial - start_time_serial
print(f"Serial Execution Time: {serial_execution_time:.4f} seconds")

start_time_parallel = time.time()
with concurrent.futures.ProcessPoolExecutor() as executor:
    future_result = executor.submit(run_kmeans_parallel, X_scaled, [6])
    y_kmeans_parallel, cluster_centers_parallel = future_result.result()[0]
end_time_parallel = time.time()

parallel_execution_time = end_time_parallel - start_time_parallel
print(f"Parallel Execution Time: {parallel_execution_time:.4f} seconds")

import time
import concurrent.futures
from sklearn.cluster import KMeans
import pandas as pd

# Load and preprocess data
dataset = pd.read_csv("Crime_Data_from_2020_to_Present.csv")
X = dataset[['Vict Age', 'LAT', 'LON']].dropna().values  # Drop rows with NaN values

# Serial KMeans function
def run_kmeans_serial(X, n_clusters, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    y_kmeans = kmeans.fit_predict(X)
    return y_kmeans, kmeans.cluster_centers_

# Parallel KMeans function
def run_kmeans(X, n_clusters, init='k-means++', random_state=42):
    kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=random_state)
    kmeans.fit(X)
    return kmeans.labels_, kmeans.cluster_centers_

# Parallel execution of KMeans using concurrent.futures
def run_kmeans_parallel(X, n_clusters_list, init='k-means++', random_state=42):
    if isinstance(n_clusters_list, int):
        n_clusters_list = [n_clusters_list]

    results = []
    with concurrent.futures.ProcessPoolExecutor() as executor:
        futures = [
            executor.submit(run_kmeans, X, n_clusters, init, random_state)
            for n_clusters in n_clusters_list
        ]
        for future in concurrent.futures.as_completed(futures):
            results.append(future.result())

    return results

# Measure serial execution time
start_time_serial = time.time()
y_kmeans_serial, cluster_centers_serial = run_kmeans_serial(X, 6, init='k-means++', random_state=42)
end_time_serial = time.time()
serial_execution_time = end_time_serial - start_time_serial
print(f"Serial Execution Time: {serial_execution_time:.4f} seconds")

# Measure parallel execution time for multiple cluster sizes
start_time_parallel = time.time()
with concurrent.futures.ProcessPoolExecutor() as executor:
    # Try running multiple cluster configurations to fully utilize parallel processing
    future_result = executor.submit(run_kmeans_parallel, X, [2])
    parallel_results = future_result.result()

    # Extract results for n_clusters=6 (first entry in list)
    y_kmeans_parallel, cluster_centers_parallel = parallel_results[0]

end_time_parallel = time.time()
parallel_execution_time = end_time_parallel - start_time_parallel
print(f"Parallel Execution Time: {parallel_execution_time:.4f} seconds")

# Calculate and display speedup ratio
speedup_ratio = serial_execution_time / parallel_execution_time
print(f"Speedup Ratio: {speedup_ratio:.2f}")

#function to find serial time
def calculate_Serial_Time(num_clusters):
  start_time_serial = time.time()
  y_kmeans_serial, cluster_centers_serial = run_kmeans_serial(X_scaled, num_clusters, init='k-means++', random_state=42)
  end_time_serial = time.time()

  serial_execution_time = end_time_serial - start_time_serial

  return serial_execution_time

#function to find parallel time
def calculate_Parallel_Time(num_clusters):
  start_time_parallel = time.time()
  with concurrent.futures.ProcessPoolExecutor() as executor:
    future_result = executor.submit(run_kmeans_parallel, X_scaled, num_clusters, init='k-means++', random_state=42)
    y_kmeans_parallel, cluster_centers_parallel = future_result.result()[0]
    end_time_parallel = time.time()

    parallel_execution_time = end_time_parallel - start_time_parallel
    return parallel_execution_time

from concurrent.futures import ProcessPoolExecutor
import time

# Measure serial execution time
def calculate_Serial_Time(num_clusters):
 start_time_serial = time.time()
 y_kmeans_serial, cluster_centers_serial = run_kmeans_serial(X_scaled, 6, init='k-means++', random_state=42)
 end_time_serial = time.time()
 serial_execution_time = end_time_serial - start_time_serial
 print(f"Serial Execution Time: {serial_execution_time:.4f} seconds")

# Measure parallel execution time with optimized settings
def calculate_Parallel_Time(num_clusters):
 start_time_parallel = time.time()
 with ProcessPoolExecutor(max_workers=4) as executor:  # Adjust max_workers as needed
  future_result = executor.submit(run_kmeans_parallel, X_scaled, [6])
  y_kmeans_parallel, cluster_centers_parallel = future_result.result()[0]
  end_time_parallel = time.time()
  parallel_execution_time = end_time_parallel - start_time_parallel
  print(f"Parallel Execution Time: {parallel_execution_time:.4f} seconds")

# import time
# #number of clusters
# l1=[1,2,3,4,5,6,7,8,9,10]
# l2=[]

# for i in l1:
#   serial_Time=calculate_Serial_Time(i)
#   parallel_time=calculate_Parallel_Time(i)
#   speedup_Ratio=serial_Time/parallel_time
#   l2.append(speedup_Ratio)

import time
from concurrent.futures import ProcessPoolExecutor

# List of cluster numbers to test
l1 = [1, 2, 3, 4, 5,6,7,8,9,10]
speedup_ratios = []

def calculate_Serial_Time(n_clusters):
    start_time_serial = time.time()
    y_kmeans_serial, cluster_centers_serial = run_kmeans_serial(X_scaled, n_clusters, init='k-means++', random_state=42)
    end_time_serial = time.time()
    return end_time_serial - start_time_serial

def calculate_Parallel_Time(n_clusters):
    start_time_parallel = time.time()
    with ProcessPoolExecutor(max_workers=2) as executor:
        # Adjust max_workers to optimize performance
        future_result = executor.submit(run_kmeans_parallel, X_scaled, [n_clusters])
        y_kmeans_parallel, cluster_centers_parallel = future_result.result()[0]
    end_time_parallel = time.time()
    return end_time_parallel - start_time_parallel

# Loop through each number of clusters in l1
for n_clusters in l1:
    serial_time = calculate_Serial_Time(n_clusters)
    parallel_time = calculate_Parallel_Time(n_clusters)

    # Calculate the speedup ratio
    speedup_ratio = serial_time / parallel_time if parallel_time > 0 else float('inf')
    speedup_ratios.append(speedup_ratio)

# Output the speedup ratios for each cluster count
for clusters, speedup in zip(l1, speedup_ratios):
    print(f"Number of clusters: {clusters}, Speedup Ratio: {speedup:.4f}")

import matplotlib.pyplot as plt


l1=l1[:5]
speedup_ratios=speedup_ratios[:5]
# Plot the speedup ratios
plt.plot(l1,speedup_ratios)
plt.xlabel("Number_Of_Clusters")
plt.ylabel("Speedup")
plt.title("Serial VS Parallel Execution")

plt.show()